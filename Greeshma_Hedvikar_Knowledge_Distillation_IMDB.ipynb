{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "YpQU38D4zdiR",
        "outputId": "9cbdfc3c-ca07-48da-f502-681a28622ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/18.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/18.3 MB\u001b[0m \u001b[31m203.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12.9/18.3 MB\u001b[0m \u001b[31m174.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m226.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.16.2+cpu requires torch==2.1.2, which is not installed.\n",
            "accelerate 1.5.2 requires torch>=2.0.0, which is not installed.\n",
            "sentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\n",
            "peft 0.14.0 requires torch>=1.13.0, which is not installed.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.2\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.1.2%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
            "Collecting torchvision==0.16.2\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.16.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "Collecting torchaudio==2.1.2\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.1.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "Requirement already satisfied: torchtext==0.16.2 in /usr/local/lib/python3.11/dist-packages (0.16.2+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2024.12.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2) (11.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.16.2) (4.67.1)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.16.2) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1->torchtext==0.16.2) (2.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2) (1.3.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-2.1.2+cu118 torchaudio-2.1.2+cu118 torchvision-0.16.2+cu118\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchvision"
                ]
              },
              "id": "c4b58eb88935458da3167d3cc5d94089"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete.\n",
            "\n",
            "Please RESTART the runtime manually to apply changes:\n",
            "1. Go to the top menu: Runtime → Restart Runtime\n",
            "2. After the restart, execute the notebook from Cell 2 onwards.\n",
            "\n",
            "All necessary installations have been completed and applied in Cell 1, thus post session-restart, start the execution from Cell 2.\n"
          ]
        }
      ],
      "source": [
        "#Installation\n",
        "!pip uninstall -y numpy torch torchvision torchaudio -q\n",
        "!pip install numpy==1.26.4 --no-cache-dir --force-reinstall -q\n",
        "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 torchtext==0.16.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers datasets scikit-learn -q\n",
        "\n",
        "print(\n",
        "    \"Setup complete.\\n\\n\"\n",
        "    \"Please RESTART the runtime manually to apply changes:\\n\"\n",
        "    \"1. Go to the top menu: Runtime → Restart Runtime\\n\"\n",
        "    \"2. After the restart, execute the notebook from Cell 2 onwards.\\n\\n\"\n",
        "    \"All necessary installations have been completed and applied in Cell 1, thus post session-restart, start the execution from Cell 2.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import softmax, log_softmax\n",
        "from torchtext.vocab import GloVe\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "ihMiOxP5zjS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3190e6d6-2f5b-480e-9fc5-f57525861c52"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading IMDb dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Shuffling train set for randomness\n",
        "train_data = dataset[\"train\"].shuffle(seed=42)\n",
        "test_data = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "dvETJnC10HIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3144e55e-cd1f-42ca-a784-c172713e7732"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "train_data = train_data.map(tokenize_function, batched=True)\n",
        "test_data = test_data.map(tokenize_function, batched=True)\n",
        "\n",
        "train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
      ],
      "metadata": {
        "id": "96dzNIOA0IxN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds)\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=teacher_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "fBomXbZO0KRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3d1de8-ed6a-4a17-da4f-29556dc50a1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "VyFIdvKKmIBm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "078e65f8-5c81-4af4-f662-69c92c084892"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1563/1563 11:41, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.289200</td>\n",
              "      <td>0.219231</td>\n",
              "      <td>0.912600</td>\n",
              "      <td>0.912868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1563, training_loss=0.28923205511736244, metrics={'train_runtime': 703.0461, 'train_samples_per_second': 35.56, 'train_steps_per_second': 2.223, 'total_flos': 1655842483200000.0, 'train_loss': 0.28923205511736244, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model.save_pretrained(\"my_teacher_model\")\n",
        "tokenizer.save_pretrained(\"my_teacher_model\")"
      ],
      "metadata": {
        "id": "1HjRoZYb6A01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfef97f-85b4-4844-cdc4-6ea1ca53385c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('my_teacher_model/tokenizer_config.json',\n",
              " 'my_teacher_model/special_tokens_map.json',\n",
              " 'my_teacher_model/vocab.txt',\n",
              " 'my_teacher_model/added_tokens.json',\n",
              " 'my_teacher_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Teacher Logits\n",
        "def get_logits(model, dataloader):\n",
        "    model.eval()\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits_list.append(outputs.logits.cpu())\n",
        "            labels_list.append(labels.cpu())\n",
        "\n",
        "    return torch.cat(logits_list), torch.cat(labels_list)\n",
        "\n",
        "teacher_loader = DataLoader(train_data, batch_size=32)\n",
        "teacher_logits, true_labels = get_logits(teacher_model, teacher_loader)"
      ],
      "metadata": {
        "id": "k_yB9qhs0MVa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(teacher_logits, \"teacher_logits.pt\")"
      ],
      "metadata": {
        "id": "k3NQGBCmHS0b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove = GloVe(name=\"6B\", dim=300)"
      ],
      "metadata": {
        "id": "BF-1Ap4TzBD-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "vocab_size = tokenizer.vocab_size\n",
        "embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for i in range(vocab_size):\n",
        "    token = tokenizer.convert_ids_to_tokens(i)\n",
        "    if token in glove.stoi:\n",
        "        embedding_matrix[i] = glove[token]\n",
        "    else:\n",
        "        embedding_matrix[i] = torch.randn(embedding_dim) * 0.05"
      ],
      "metadata": {
        "id": "ZkAA9ivUtWEB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class StudentCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_matrix, output_dim=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(300, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv1d(300, 128, kernel_size=4, padding=2)\n",
        "        self.conv5 = nn.Conv1d(300, 128, kernel_size=5, padding=2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.norm = nn.LayerNorm(128 * 3)\n",
        "        self.fc = nn.Linear(128 * 3, output_dim)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        x3 = F.relu(self.conv3(x))\n",
        "        x4 = F.relu(self.conv4(x))\n",
        "        x5 = F.relu(self.conv5(x))\n",
        "\n",
        "        x3 = torch.max(x3, dim=2).values\n",
        "        x4 = torch.max(x4, dim=2).values\n",
        "        x5 = torch.max(x5, dim=2).values\n",
        "\n",
        "        x_cat = torch.cat((x3, x4, x5), dim=1)\n",
        "        x_cat = self.norm(x_cat)\n",
        "        x_cat = self.dropout(x_cat)\n",
        "        return self.fc(x_cat)"
      ],
      "metadata": {
        "id": "VPPneWvq0PPG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = StudentCNN(vocab_size, embedding_matrix).to(device)\n",
        "\n",
        "criterion_ce = nn.CrossEntropyLoss()\n",
        "criterion_kd = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-3)\n",
        "temperature = 2.0\n",
        "\n",
        "student_model.train()\n",
        "for epoch in range(1):\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(teacher_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        soft_targets = softmax(teacher_logits[i*32:(i+1)*32].to(device) / temperature, dim=1)\n",
        "        student_logits = student_model(input_ids)\n",
        "        soft_logits = log_softmax(student_logits / temperature, dim=1)\n",
        "\n",
        "        loss_ce = criterion_ce(student_logits, labels)\n",
        "        loss_kd = criterion_kd(soft_logits, soft_targets) * (temperature ** 2)\n",
        "        loss = 0.3 * loss_ce + 0.7 * loss_kd\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "5cCh6_oS0Q0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "116814ba-abf8-4d41-f287-0d31c583099b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 310.5682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Evaluation of Teacher and Student\n",
        "def evaluate(model, dataset):\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=32)\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            if hasattr(model, \"config\"):\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "            else:\n",
        "                logits = model(input_ids)\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    return acc, f1\n",
        "\n",
        "# Evaluate\n",
        "teacher_acc, teacher_f1 = evaluate(teacher_model, test_data)\n",
        "student_acc, student_f1 = evaluate(student_model, test_data)\n",
        "\n",
        "print(f\"Teacher Accuracy: {teacher_acc:.4f}, F1 Score: {teacher_f1:.4f}\")\n",
        "print(f\"Student Accuracy: {student_acc:.4f}, F1 Score: {student_f1:.4f}\")"
      ],
      "metadata": {
        "id": "dnwLPWDz0UjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6e2371-f06a-4033-f2dd-fa5af1b48b63"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Accuracy: 0.9126, F1 Score: 0.9129\n",
            "Student Accuracy: 0.8666, F1 Score: 0.8576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained student model\n",
        "torch.save(student_model.state_dict(), \"student_cnn.pt\")"
      ],
      "metadata": {
        "id": "2gmVEwOBAPxR"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}